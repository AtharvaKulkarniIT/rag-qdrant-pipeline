{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNeQN0GPv1dLRk5fF9DJUHh"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Install Required Packages**\n"
      ],
      "metadata": {
        "id": "GGdaB0Cxphe1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iQlbUG13pftv"
      },
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "!pip install langchain PyMuPDF\n",
        "!pip install langchain_google_genai\n",
        "!pip install sentence-transformers\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Import Libraries**"
      ],
      "metadata": {
        "id": "TCki3mjNpq-U"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "POT8K6EfpqyY"
      },
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "from langchain.embeddings import SentenceTransformerEmbeddings\n",
        "from langchain.vectorstores import Qdrant\n",
        "import os\n",
        "import numpy as np\n",
        "import google.generativeai as genai\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain.chains.question_answering import load_qa_chain\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.document_loaders import DirectoryLoader, PyMuPDFLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Load PDF Files**"
      ],
      "metadata": {
        "id": "_ouqseKcpxvN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aCW8WXkbp11r"
      },
      "outputs": [],
      "source": [
        "# Specify the data folder path\n",
        "DATA_FOLDER_PATH = \"/content/drive/MyDrive/data\"\n",
        "\n",
        "# Load PDF files from the specified folder\n",
        "loader = DirectoryLoader(DATA_FOLDER_PATH, glob=\"**/*.pdf\", loader_cls=PyMuPDFLoader, show_progress=True)\n",
        "documents = loader.load()\n",
        "\n",
        "# Split loaded documents into chunks with chunk size of 1000 characters and 40 characters overlap\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=40)\n",
        "docs = text_splitter.split_documents(documents)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Initialize SentenceTransformer Embedding Model**"
      ],
      "metadata": {
        "id": "ZMcHPAu9p6A6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U4RLG_vmp_dR"
      },
      "outputs": [],
      "source": [
        "embeddings = SentenceTransformerEmbeddings(model_name=\"all-Mpnet-base-v2\")  # Use a model that produces 768 dimensions\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Configure Qdrant**"
      ],
      "metadata": {
        "id": "Gb4JsJcYqDBd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XBCx31MAqIRy"
      },
      "outputs": [],
      "source": [
        "# Qdrant Cloud Configuration\n",
        "qdrant_cloud_api_key = \"qdrant_api\"\n",
        "qdrant_url = \"qdrant_url\"\n",
        "\n",
        "qdrant_cloud = Qdrant.from_documents(\n",
        "    docs,\n",
        "    embeddings,\n",
        "    url=qdrant_url,\n",
        "    prefer_grpc=True,\n",
        "    api_key=qdrant_cloud_api_key,\n",
        "    collection_name=\"new_docs\",\n",
        "    force_recreate=True\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Configure Gemini API**"
      ],
      "metadata": {
        "id": "vZQiL1AVqLUc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K1qOZ2CMqRYs"
      },
      "outputs": [],
      "source": [
        "# Set the Gemini API key from the environment variable\n",
        "gemini_api_key = 'replace_with_your_api'\n",
        "if not gemini_api_key:\n",
        "    raise ValueError(\"Gemini API key not found. Please set the GEMINI API KEY \")\n",
        "\n",
        "genai.configure(api_key=gemini_api_key)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Define functions**"
      ],
      "metadata": {
        "id": "eRuLB_FqqdnH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iJ2T_a2wqjeb"
      },
      "outputs": [],
      "source": [
        "# Function to get the conversational QA chain using the Gemini model\n",
        "def get_conversational_chain():\n",
        "    prompt_template1 = \"\"\"\n",
        "    Answer the question as thoroughly as possible based on the provided context. Analyze the relationships within the content and generate meaningful, context-aware bullet points. If the answer is not available in the provided context, simply state, 'Answer is not available in the context.' Avoid providing incorrect information.\n",
        "    Answer:\n",
        "    \"\"\"\n",
        "    model = ChatGoogleGenerativeAI(model=\"gemini-pro\", temperature=0.3, api_key=gemini_api_key)\n",
        "    prompt = PromptTemplate(template=prompt_template1, input_variables=[\"context\", \"question\"])\n",
        "    chain = load_qa_chain(model, chain_type=\"stuff\", prompt=prompt)\n",
        "    return chain\n",
        "\n",
        "# Function to interact with Gemini's generative AI\n",
        "def get_gemini_response(input_text):\n",
        "    # Create the Gemini model\n",
        "    model = genai.GenerativeModel('gemini-pro')\n",
        "\n",
        "    # Generate a response using the model's generate_content method\n",
        "    response = model.generate_content(input_text)\n",
        "    print(\"\\n\\n Summary:\\n\\n\")\n",
        "    try {\n",
        "        # Access the 'candidates' list and extract the 'text' from the first candidate\n",
        "        content = response.candidates[0].content.parts[0].text\n",
        "        return content\n",
        "    } except (AttributeError, Exception as e) {\n",
        "        print(f\"An error occurred: {e}\")\n",
        "        return \"Error: Unable to retrieve response content.\"\n",
        "    }\n",
        "\n",
        "# Function to perform the hybrid search with RRF\n",
        "def hybrid_search(query, qdrant_store, documents, k=5):\n",
        "    # Perform similarity search in Qdrant\n",
        "    found_docs_qdrant = qdrant_store.similarity_search(query)\n",
        "\n",
        "    # Perform keyword search in the loaded documents\n",
        "    found_docs_keyword = [doc for doc in documents if query.lower() in doc.page_content.lower()]\n",
        "\n",
        "    # Combine results from both searches\n",
        "    combined_docs = found_docs_qdrant + found_docs_keyword[:k]\n",
        "\n",
        "    # Get ranks for reciprocal rank fusion (RRF)\n",
        "    ranks = {doc.metadata['id']: idx for idx, doc in enumerate(combined_docs)}\n",
        "\n",
        "    # Sort combined documents based on RRF\n",
        "    combined_docs_sorted = sorted(combined_docs, key=lambda doc: ranks[doc.metadata['id']])\n",
        "\n",
        "    return combined_docs_sorted[:k]\n"
      ]
    }
  ]
}
