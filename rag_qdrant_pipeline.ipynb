{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNeQN0GPv1dLRk5fF9DJUHh"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Install Required Packages**\n"
      ],
      "metadata": {
        "id": "GGdaB0Cxphe1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iQlbUG13pftv"
      },
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "!pip install langchain PyMuPDF\n",
        "!pip install langchain_google_genai\n",
        "!pip install sentence-transformers\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Import Libraries**"
      ],
      "metadata": {
        "id": "TCki3mjNpq-U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "from langchain.embeddings import SentenceTransformerEmbeddings\n",
        "from langchain.vectorstores import Qdrant\n",
        "import os\n",
        "import numpy as np\n",
        "import google.generativeai as genai\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain.chains.question_answering import load_qa_chain\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.document_loaders import DirectoryLoader, PyMuPDFLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n"
      ],
      "metadata": {
        "id": "POT8K6EfpqyY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Load PDF Files**"
      ],
      "metadata": {
        "id": "_ouqseKcpxvN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Specify the data folder path\n",
        "DATA_FOLDER_PATH = \"/content/drive/MyDrive/data\"\n",
        "\n",
        "# Load PDF files from the specified folder\n",
        "loader = DirectoryLoader(DATA_FOLDER_PATH, glob=\"**/*.pdf\", loader_cls=PyMuPDFLoader, show_progress=True)\n",
        "documents = loader.load()\n",
        "\n",
        "# Split loaded documents into chunks with chunk size of 1000 characters and 40 characters overlap\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=40)\n",
        "docs = text_splitter.split_documents(documents)\n"
      ],
      "metadata": {
        "id": "aCW8WXkbp11r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Initialize SentenceTransformer Embedding Model**"
      ],
      "metadata": {
        "id": "ZMcHPAu9p6A6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings = SentenceTransformerEmbeddings(model_name=\"all-Mpnet-base-v2\")  # Use a model that produces 768 dimensions\n"
      ],
      "metadata": {
        "id": "U4RLG_vmp_dR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Configure Qdrant**"
      ],
      "metadata": {
        "id": "Gb4JsJcYqDBd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Qdrant Cloud Configuration\n",
        "qdrant_cloud_api_key = \"qdrant_api\"\n",
        "qdrant_url = \"qdrant_url\"\n",
        "\n",
        "qdrant_cloud = Qdrant.from_documents(\n",
        "    docs,\n",
        "    embeddings,\n",
        "    url=qdrant_url,\n",
        "    prefer_grpc=True,\n",
        "    api_key=qdrant_cloud_api_key,\n",
        "    collection_name=\"new_docs\",\n",
        "    force_recreate=True,\n",
        ")\n"
      ],
      "metadata": {
        "id": "XBCx31MAqIRy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Configure Gemini API**"
      ],
      "metadata": {
        "id": "vZQiL1AVqLUc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the Gemini API key from the environment variable\n",
        "gemini_api_key = 'replace_with_your_api'\n",
        "if not gemini_api_key:\n",
        "    raise ValueError(\"Gemini API key not found. Please set the GEMINI API KEY \")\n",
        "\n",
        "genai.configure(api_key=gemini_api_key)\n"
      ],
      "metadata": {
        "id": "K1qOZ2CMqRYs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Define functions**"
      ],
      "metadata": {
        "id": "eRuLB_FqqdnH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to get the conversational QA chain using the Gemini model\n",
        "def get_conversational_chain():\n",
        "    prompt_template1 = \"\"\"\n",
        "    Answer the question as detailed as possible from the provided context. Check for the relationships in the content. Generate meaningful context-aware bullet points. If the answer is not in\n",
        "    the provided context, just say \"answer is not available in the context.\" Do not provide a wrong answer.\\n\\n\n",
        "    Context:\\n {context}\\n\n",
        "    Question:\\n {question}\\n\n",
        "\n",
        "    Answer:\n",
        "    \"\"\"\n",
        "\n",
        "    model = ChatGoogleGenerativeAI(model=\"gemini-pro\", temperature=0.3, api_key=gemini_api_key)\n",
        "    prompt = PromptTemplate(template=prompt_template1, input_variables=[\"context\", \"question\"])\n",
        "    chain = load_qa_chain(model, chain_type=\"stuff\", prompt=prompt)\n",
        "\n",
        "    return chain\n",
        "\n",
        "# Function to interact with Gemini's generative AI\n",
        "def get_gemini_response(input_text):\n",
        "    # Create the Gemini model\n",
        "    model = genai.GenerativeModel('gemini-pro')\n",
        "\n",
        "    # Generate a response using the model's generate_content method\n",
        "    response = model.generate_content(input_text)\n",
        "    print(\"\\n\\n Summary:\\n\\n\")\n",
        "    try:\n",
        "        # Access the 'candidates' list and extract the 'text' from the first candidate\n",
        "        content = response.candidates[0].content.parts[0].text\n",
        "        return content\n",
        "    except AttributeError as e:\n",
        "        # Handle attribute errors in case of incorrect structure\n",
        "        print(f\"Attribute error: {e}\")\n",
        "        return \"Error: Unable to retrieve response content.\"\n",
        "    except Exception as e:\n",
        "        # Handle any other exceptions\n",
        "        print(f\"An error occurred: {e}\")\n",
        "        return \"Error: Something went wrong.\"\n",
        "\n",
        "# Function to perform the hybrid search with RRF\n",
        "def hybrid_search(query, qdrant_store, documents, k=5):\n",
        "    # Perform similarity search in Qdrant\n",
        "    found_docs_qdrant = qdrant_store.similarity_search(query)\n",
        "\n",
        "    # Perform keyword search in the loaded documents\n",
        "    found_docs_keyword = [doc for doc in documents if query.lower() in doc.page_content.lower()]\n",
        "\n",
        "    # Combine results from both searches\n",
        "    combined_docs = found_docs_qdrant + found_docs_keyword[:k]  # Limit keyword search results to k\n",
        "\n",
        "    # Get ranks for reciprocal rank fusion (RRF)\n",
        "    ranks_qdrant = np.array([1 / (i + 1) for i in range(len(found_docs_qdrant))])\n",
        "    ranks_keyword = np.array([1 / (i + 1) for i in range(len(found_docs_keyword[:k]))])\n",
        "\n",
        "    # Create a combined list of scores\n",
        "    combined_scores = np.concatenate((ranks_qdrant, ranks_keyword))\n",
        "\n",
        "    # Get the indices of the sorted scores in descending order\n",
        "    sorted_indices = np.argsort(-combined_scores)\n",
        "\n",
        "    # Sort the combined documents based on the sorted indices\n",
        "    sorted_docs = [combined_docs[i] for i in sorted_indices]\n",
        "\n",
        "    return sorted_docs\n"
      ],
      "metadata": {
        "id": "iJ2T_a2wqjeb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Main Function to Process the Query**"
      ],
      "metadata": {
        "id": "WSQxn0JOqknI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Main function to process the query, perform hybrid search and generate responses\n",
        "def main():\n",
        "    # Define a query for hybrid search\n",
        "    query = \"Describe the rivers in Maharashtra\"\n",
        "\n",
        "    # Perform hybrid search\n",
        "    hybrid_results = hybrid_search(query, qdrant_cloud, docs, k=5)\n",
        "\n",
        "    # Display the results of hybrid search\n",
        "    print(f\"\\n\\nNumber of relevant documents: {len(hybrid_results)}\")\n",
        "    if hybrid_results:\n",
        "        print(\"Retrieved Content:\")\n",
        "        print(hybrid_results[0].page_content)\n",
        "\n",
        "# Call the main function\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "lg4duepOqoUv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
